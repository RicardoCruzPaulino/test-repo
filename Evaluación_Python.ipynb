{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicardoCruzPaulino/test-repo/blob/master/Evaluaci%C3%B3n_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3UYA16DDQ3-"
      },
      "source": [
        "\n",
        "<h2><font color=\"#004D7F\" size=4>Módulo 4</font></h2>\n",
        "\n",
        "\n",
        "\n",
        "<h1><font color=\"#004D7F\" size=5>Evaluación conocimientos</font></h1>\n",
        "\n",
        "<br>\n",
        "<div style=\"text-align: right\">\n",
        "<font color=\"#004D7F\" size=3>Asociación Popular de Ahorro y Préstamos - APAP</font><br>\n",
        "<font color=\"#004D7F\" size=3>Prevención y Contro de Fraude\n",
        "</font><br>\n",
        "<font color=\"#004D7F\" size=3>Setiembre 2023</font>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWij6XOkDQ4J"
      },
      "source": [
        "---\n",
        "\n",
        "<a id=\"indice\"></a>\n",
        "<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n",
        "\n",
        "\n",
        "* [1. Introducción](#section1)\n",
        "* [2. Preparando los datos](#section2)\n",
        "   * [Análisis de datos](#section21)\n",
        "   * [Preprocesamiento](#section22)\n",
        "* [3. Fase de modelado](#section3)\n",
        "   * [Gradiente descendiente estocástico (SGD)](#section33)\n",
        "   * [Regresión logística](#section31)\n",
        "   * [Support Vector Machine (SVM)](#section32)\n",
        "   * [Árboles de decision](#section34)\n",
        "* [4. Conclusiones](#section5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQVAZTzUDQ4L"
      },
      "source": [
        "---\n",
        "\n",
        "<a id=\"section1\"></a>\n",
        "## <font color=\"#004D7F\"> 1. Introducción</font>\n",
        "\n",
        "La presente evaluación tiene como finalidad medir el conocimiento de la herramienta y el proceso de construcción de modelos ya sea de clasificación o predictivos.\n",
        "\n",
        "En esta evaluación se va a utilizar el dataset [Census Income Dataset](http://archive.ics.uci.edu/ml/datasets/Census+Income). Este dataset contiene datos como la edad, trabajo, estudios, etc. de más de 48K personas.\n",
        "\n",
        "El objetivo consiste en predecir si dicha persona tiene unos ingresos que superan los 50K dólares anuales; para ello deberá guiarse a través de la metodología CRISP-DM para llevar a cabo el objetivo esperado.\n",
        "\n",
        "\n",
        "<div style=\"text-align: right\">\n",
        "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></font></a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oouNqeDBpHwk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meDpMtgxDQ4N"
      },
      "source": [
        "---\n",
        "\n",
        "<a id=\"section2\"></a>\n",
        "## <font color=\"#004D7F\"> 2. Preparando los datos</font>\n",
        "\n",
        "\n",
        "Cargar el dataset indicado y las librerias necesarias para analizar los datos de entrada. Se solicita:\n",
        "\n",
        "* ¿Que tipos de datos encontramos en el dataset?\n",
        "* ¿La variable respuesta esta balanceada?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uv6Oxt6dDQ4P"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, LabelBinarizer, MultiLabelBinarizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Cargar el dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "column_names = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \"occupation\",\n",
        "                \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"]\n",
        "data = pd.read_csv(url, names=column_names, sep=',\\s', engine='python')\n",
        "\n",
        "url_test = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
        "\n",
        "test_data = pd.read_csv(url_test, names=column_names, sep=',\\s', engine='python')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clases y Funciones Utilitarias\n",
        "\n",
        "\n",
        "#Clases Utilitarias de Transformaciones\n",
        "\n",
        "\n",
        "class LabelBinarizerForPipeline(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Extends LabelBinarizer to work in pipelines for both single and multi-output scenarios.\n",
        "    \"\"\"\n",
        "    def __init__(self, sparse_output=False):\n",
        "        self.sparse_output = sparse_output\n",
        "        self.encoder = None  # Initialize encoder as None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fits the encoder based on the shape of the input data.\n",
        "        Uses MultiLabelBinarizer for 2D input (multi-output) and LabelBinarizer for 1D input (single-output).\n",
        "        \"\"\"\n",
        "        if X.ndim == 2:  # Check if input is 2D (multi-output)\n",
        "            self.encoder = MultiLabelBinarizer(sparse_output=self.sparse_output)\n",
        "        else:\n",
        "            self.encoder = LabelBinarizer(sparse_output=self.sparse_output)\n",
        "        self.encoder.fit(X)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Transforms the input data using the fitted encoder.\"\"\"\n",
        "        return self.encoder.transform(X)\n",
        "\n",
        "\n",
        "# Clase personalizada para preprocesamiento\n",
        "class DataPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, numeric_cols, categorical_cols):\n",
        "        \"\"\"\n",
        "        Inicializa el preprocesador con las columnas numéricas y categóricas.\n",
        "\n",
        "        Args:\n",
        "            numeric_cols (list): Lista de nombres de columnas numéricas\n",
        "            categorical_cols (list): Lista de nombres de columnas categóricas\n",
        "        \"\"\"\n",
        "        self.numeric_cols = numeric_cols\n",
        "        self.categorical_cols = categorical_cols\n",
        "        self.preprocessor = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Ajusta el preprocesador a los datos.\"\"\"\n",
        "        # Definimos el ColumnTransformer con los transformadores específicos\n",
        "        self.preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', StandardScaler(), self.numeric_cols),          # Escala datos numéricos\n",
        "           #     ('cat', LabelBinarizerForPipeline(), self.categorical_cols)       # Binariza datos categóricos\n",
        "\n",
        "                 (\"onehot\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), self.categorical_cols)\n",
        "            ],\n",
        "            remainder='passthrough'  # Mantiene las columnas no especificadas sin cambios\n",
        "        )\n",
        "        self.preprocessor.fit(X,y)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transforma los datos usando el preprocesador ajustado.\"\"\"\n",
        "        return self.preprocessor.transform(X)\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Ajusta y transforma los datos en un solo paso.\"\"\"\n",
        "        return self.fit(X, y).transform(X)\n",
        "\n",
        "\n",
        "###  Funciones\n",
        "def evaluate_model(name, model, y_test, y_pred):\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Model: {name}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}\\n\")\n",
        "\n",
        "# Evaluación del mejor modelo en el conjunto de prueba\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    #return accuracy, precision, recall, f1, roc_auc\n",
        "\n",
        "\n",
        "\n",
        "def handle_missing_values(data):\n",
        "  # Reemplazar valores nulos por NaN\n",
        "  data.replace('?', np.nan, inplace=True)\n",
        "  # Imputar valores perdidos en variables numéricas con la mediana\n",
        "  numeric_imputer = SimpleImputer(strategy='median')\n",
        "  data[['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']] = numeric_imputer.fit_transform(data[['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']])\n",
        "\n",
        "  # Imputar valores perdidos en variables categóricas con la moda\n",
        "  categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "  data[['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']] = categorical_imputer.fit_transform(data[['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']])\n",
        "  return data\n",
        "\n",
        "def replace_outliers_with_mean(data):\n",
        "  for column in data.select_dtypes(include=['int64', 'float64']).columns:\n",
        "    z_scores = stats.zscore(data[column])\n",
        "    abs_z_scores = np.abs(z_scores)\n",
        "    filtered_entries = (abs_z_scores < 3)\n",
        "    data[column] = data[column].where(filtered_entries, data[column].mean())\n",
        "    return data\n",
        "\n",
        "# Aplicar la función a las columnas numéricas\n",
        "\n",
        "def plot_boxplots(data):\n",
        " for col in data.select_dtypes(include=['int64', 'float64']).columns:\n",
        "      plt.figure(figsize=(10, 4))\n",
        "      sns.boxplot(x=data[col])\n",
        "      plt.title(f'Boxplot for {col}')\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def plot_scatter_plots(data):\n",
        "  for column in data.select_dtypes(include=['int64', 'float64']).columns:\n",
        "     if column != 'income':\n",
        "      plt.figure(figsize=(10, 6))\n",
        "      sns.scatterplot(x=data[column], y=data['income'])\n",
        "      plt.title(f'Scatter plot of {column} vs Income')\n",
        "      plt.xlabel(f'{column}')\n",
        "      plt.ylabel('Income')\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def plot_matriz_correlaciones(data):\n",
        "\n",
        "# Matriz de correlación\n",
        "corr = data.select_dtypes(include=['int64', 'float64']).corr()\n",
        "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "def plot_histograms(data):\n",
        "# Histogramas para variables numéricas\n",
        "for col in data.select_dtypes(include=['int64', 'float64']).columns:\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.histplot(data[col], kde=True)\n",
        "    plt.title(f'Histogram for {col}')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def EDA_data(data):\n",
        "  print(\"Dimensiones del dataset\")\n",
        "  data.shape\n",
        "  print(f\"El dataset tiene {data.shape[0]} filas y {data.shape[1]} columnas.\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "  print(\"Tipos de datos del dataset\")\n",
        "  data.dtypes\n",
        "  print(\"\\n\")\n",
        "\n",
        "  print(\"Información del dataset\")\n",
        "  data.info()\n",
        "\n",
        "  print(\"Descripción estadística\")\n",
        "  data.describe()\n",
        "\n",
        "  print(\"Primeras 5 filas del dataset\")\n",
        "  data.head()\n",
        "\n",
        "  print(\"Ultimas 5 filas del dataset\")\n",
        "  data.tail()\n",
        "\n",
        "\n",
        "  print(\"Valores nulos en el dataset\")\n",
        "  data.isnull().sum()\n",
        "  print(\"\\n\")\n",
        "\n",
        "  print(\"Distribución de la variable objetivo\")\n",
        "  data['income'].value_counts()\n",
        "  print(\"\\n\")\n",
        "  print(\"Proporción de cada clase\")\n",
        "  data['income'].value_counts(normalize=True)\n",
        "  print(\"\\n\")\n",
        "\n",
        "  print(\"Distribución de las variables categóricas\")\n",
        "  data.select_dtypes(include=['object']).describe()\n",
        "  print(\"\\n\")\n",
        "\n",
        "  print(\"Distribución de las variables numéricas\")\n",
        "  data.select_dtypes(include=['int64', 'float64']).describe()\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "  # 1. Boxplots para identificar outliers univariados\n",
        "  print(\"# 1. Boxplots para identificar outliers univariados\")\n",
        "  plot_boxplots(data)\n",
        "\n",
        "\n",
        "  print(\"# 2. Histogramas para identificar outliers univariados\")\n",
        "  # 2. Histogramas para identificar outliers univariados\n",
        "  plot_histograms(data)\n",
        "\n",
        "\n",
        "  print(\"# 3. Scatter plots para identificar relaciones multivariadas \")\n",
        "  # 3. Scatter plots para identificar relaciones multivariadas\n",
        "  plot_scatter_plots(data)\n",
        "\n",
        "  print(\"# 4. Matriz de correlación para identificar relaciones inusuales\")\n",
        "  # 4. Matriz de correlación para identificar relaciones inusuales\n",
        "    plot_matrix_correlaciones(data)\n",
        "\n",
        "\n",
        "def age_binning(data):\n",
        "  bins = np.linspace(min(data[\"age\"]), max(data[\"age\"]), 4)\n",
        "  bins\n",
        "  group_names = ['Young', 'Adult', 'Elder']\n",
        "\n",
        "  data['age-binned'] = pd.cut(data['age'], bins, labels=group_names, include_lowest=True )\n",
        "  data[['age','age-binned']].head(20)\n",
        "  return data\n",
        "\n",
        "def plot_tabla_contingencia(data, column):\n",
        "# Variables categóricas\n",
        "  categorical_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "  # Crear tablas de contingencia\n",
        "  for column in categorical_columns:\n",
        "      if column != 'income':\n",
        "          contingency_table = pd.crosstab(data[column], data['income'])\n",
        "          print(f\"Tabla de Contingencia para {column} y income:\")\n",
        "          print(contingency_table)\n",
        "          print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# Balancear Muestra aplicando UnderSampling datos en conjuntos de entrenamiento y prueba\n",
        "def UnderSampling_data(X, y, sampling_strategy = 0.8, test_size=0.3):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "  random_under_sampler = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
        "  X_train_res, y_train_res = random_under_sampler.fit_resample(X_train, y_train)\n",
        "  X_test_res, y_test_res = random_under_sampler.fit_resample(X_test, y_test)\n",
        "  return  X_train_res,  X_test_res, y_train_res, y_test_res\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def best_model(results_models):\n",
        "  \"\"\"\n",
        "  This function selects the best model from a list of model results based on the test score.\n",
        "\n",
        "  Args:\n",
        "    results_models: A list of lists, where each inner list represents a model\n",
        "      and contains [model_name, best_params, best_cv_score, test_score].\n",
        "\n",
        "  Returns:\n",
        "    A list containing the name and test score of the best model, or None if\n",
        "    the input list is empty or invalid.\n",
        "  \"\"\"\n",
        "  if not results_models or not all(isinstance(model_data, list) and len(model_data) == 4 for model_data in results_models):\n",
        "    return None\n",
        "\n",
        "  best_model_data = max(results_models, key=lambda x: x[3])  # Find the model with the highest test score\n",
        "\n",
        "  return [best_model_data[0], best_model_data[1], best_model_data[2], best_model_data[3]] # Return the best model's name and test score\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I34IqLTIKN9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **¿Que tipos de datos encontramos en el dataset?**"
      ],
      "metadata": {
        "id": "NlQ09ZukTXDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Respuesta:** Al momnento de ejecucion identificamos que los siguientes tipos datos.\n",
        "*  Numerico Entero == > int64  (6 variables)\n",
        "*   Texto ==>  object   (8 variables)"
      ],
      "metadata": {
        "id": "fW_8A4bDTYI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tipo de Datos\")\n",
        "print(data.dtypes)"
      ],
      "metadata": {
        "id": "tKYYG7GuTyYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**¿La variable respuesta esta balanceada?**"
      ],
      "metadata": {
        "id": "zAY31TYjTp7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Repsuesta:** La variable respuesta \"target\" no se encuentra balanceads .\n",
        "*  **<=50K:**  Con 24,720 valores para 75.92%\n",
        "*   **>50K:** Con 7,841 valores para 24.08%"
      ],
      "metadata": {
        "id": "67edBVYRTrO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribución de la variable objetivo\n",
        "income_counts = data['income'].value_counts()\n",
        "print(income_counts)\n",
        "\n",
        "# Proporción de cada clase\n",
        "income_proportion = income_counts / income_counts.sum()\n",
        "print(income_proportion)"
      ],
      "metadata": {
        "id": "IifZrCjNUByW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "income_counts = test_data['income'].value_counts()\n",
        "print(income_counts)\n",
        "income_counts.plot(kind=\"bar\")"
      ],
      "metadata": {
        "id": "HdFwW723UDQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O-fbCVzDQ4T"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "<a id=\"section21\"></a>\n",
        "### <font color=\"#004D7F\">Análisis de datos</font>\n",
        "\n",
        "Realizar un análisis exploratorio de los datos. Responder a las siguientes preguntas (Justifique su respuesta):\n",
        "\n",
        "* ¿Estan las muestras balanceadas?\n",
        "* ¿De qué tipo son los datos? ¿Hay que transformarlos?\n",
        "* ¿Hay valores perdidos? Lleve a cabo el tratamiento adecuado según su criterio para tratar con dichos valores. Justifique su respuesta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-A_M3E2EDQ4U"
      },
      "outputs": [],
      "source": [
        "print(data.dtypes)\n",
        "\n",
        "# Distribución de la variable objetivo\n",
        "income_counts = data['income'].value_counts()\n",
        "print(income_counts)\n",
        "\n",
        "# Proporción de cada clase\n",
        "income_proportion = income_counts / income_counts.sum()\n",
        "print(income_proportion)\n",
        "\n",
        "# Revisar los tipos de datos\n",
        "print(data.dtypes)\n",
        "\n",
        "missing_values = data.isnull().sum()\n",
        "print(missing_values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**¿Estan las muestras balanceadas?**"
      ],
      "metadata": {
        "id": "-dj2i_rDSsoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Respuesta:** Dado que la variable respuesta no esta balanceada, concluimos que las muestras no estan balanceadas. Agregamos gráfico para su comprobación. Estaremo aplicando un balanceo de muestras previo al entrenamiento."
      ],
      "metadata": {
        "id": "fsOj91kkVMGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "income_counts = data['income'].value_counts()\n",
        "print(income_counts)\n",
        "income_counts.plot(kind=\"bar\")"
      ],
      "metadata": {
        "id": "477akZxFVXLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**¿De qué tipo son los datos? ¿Hay que transformarlos?**"
      ],
      "metadata": {
        "id": "ASeWDSOFVoZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Respuesta:**\n",
        "\n",
        "**Tipos de Variables de datos**\n",
        "*   **Numéricas:** age, fnlwgt, education-num, capital-gain, capital-loss, hours-per-week.\n",
        "*   **Categóricas:** workclass, education, marital-status, occupation, relationship, race, sex, native-country.\n",
        "\n",
        "**Transformación para Variables de datos**\n",
        "*   **Numéricas:**  Aplicar Estandarizacion / Escalado, segun criterio seleccionado.\n",
        "*   **Categóricas:**   Aplicar One-Hot Encoding / Label Encoding, segun aplique para el modelo.\n"
      ],
      "metadata": {
        "id": "TWYONmhcS3sA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GepWPl51WOtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**¿Hay valores perdidos? Lleve a cabo el tratamiento adecuado según su criterio para tratar con dichos valores. Justifique su respuesta**"
      ],
      "metadata": {
        "id": "qjpzTCJ7WPE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Respuesta:**\n",
        "Al momento de realizar la ejecucion\n",
        "las siguientes variables presentan valores perdidos:\n",
        "\n",
        "1.   workclass         1836\n",
        "2.   occupation        1843\n",
        "3.   native-country     583\n",
        "\n",
        "\n",
        "Dado que son variable categoricas, las reemplazaremos con la moda."
      ],
      "metadata": {
        "id": "g0_w9OwGWRZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Revisión datos perdidos\")\n",
        "missing_values = data.isnull().sum()\n",
        "print(missing_values)\n",
        "\n",
        "print(\"Corrección de datos perdidos\")\n",
        "data = handle_missing_values(data)\n",
        "missing_values = data.isnull().sum()\n",
        "print(missing_values)\n"
      ],
      "metadata": {
        "id": "qN4fwipAWiKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Existen outliers o valores atípicos? Muestre 2 indicadores (univariados y multivariados) para identificar la existencia de dichos valores. En caso existan, aplique el tratamiento adecuado según su criterio."
      ],
      "metadata": {
        "id": "QKlyeuR2rufq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Respuesta:**\n",
        "\n",
        "Si existen valores atípicos en mas de una variable. Ver los indicadores.\n",
        "\n",
        "**Indicadores Univariados**\n",
        " Para cada variable numerica presentaremos\n",
        "\n",
        "1.   Boxplots\n",
        "2.   Histogramas\n",
        "\n",
        "**Indicadores Multivariados**\n",
        "Combinaremos la variable respuesta con todas las variables  numericas.\n",
        "\n",
        "1.   Gráficos de dispersión (scatter plots)\n",
        "2.   Matriz de correlación:\n"
      ],
      "metadata": {
        "id": "e_NSj_wDXTrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "#**Indicadores Univariados**\n",
        "print(\"**Indicadores Univariados**\")\n",
        "plot_boxplots(data)\n",
        "plot_histograms(data)\n",
        "\n",
        "#**Indicadores Multivariados**\n",
        "print(\"**Indicadores Multivariados**\")\n",
        "plot_scatter_plots(data)\n",
        "plot_matrix_correlaciones(data)\n",
        "\n",
        "\n",
        "print(\"**Tratamiento de Outliers**\")\n",
        "replace_outliers_with_mean(data)\n"
      ],
      "metadata": {
        "id": "BIyQXhxEsBLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkZt_XBmDQ4X"
      },
      "source": [
        "\n",
        "LLeve a cabo un análisis descriptivo mediante tablas y gráficos (univariados y bivariados) Ejemplo: Histogramas, matriz de correlaciones, etc. Comente sus hallazgos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "zw52aB2PDQ4Y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"**Análisis descriptivo**\")\n",
        "eda_data = data.copy()\n",
        "EDA_data(eda_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDOtsRP8DQ4Z"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> __Nota__: Si tuviesemos que imputar valores a alguna columna, se podría utilizar el valor medio o mediana para las variables numéricas y el valor más frecuente para las categóricas.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXOwMi3vDQ4Z"
      },
      "source": [
        "---\n",
        "\n",
        "<a id=\"section22\"></a>\n",
        "### <font color=\"#004D7F\">Preprocesamiento</font>\n",
        "\n",
        "Antes de realizar las transformaciones necesarias tenemos que realizar la partición entre muestras de train y test. LLeve a cabo dicha partición."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_AyjcFGDQ4Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Aplicamos Binning a la variable Age\n",
        "\n",
        "print(\"Aplicamos Binning a la variable Age \")\n",
        "data = age_binning(data)\n",
        "data.head()\n",
        "\n",
        "X = data.drop(\"income\", axis=1)\n",
        "\n",
        "print(\"Preparamos el Dataset Train\")\n",
        "# Binarizamos la variable respuesta\n",
        "print (\"Binarizamos la variable respuesta\")\n",
        "print(\"Pre Binarizar\")\n",
        "print(data[\"income\"].unique())\n",
        "data[\"income\"].unique()\n",
        "\n",
        "y = data[\"income\"].apply(lambda x: 1 if x == '>50K' else 0)\n",
        "print(\"Post Binarizar\")\n",
        "data[\"income\"].unique()\n",
        "# Dividir datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6EbSeENDQ4a"
      },
      "source": [
        "¿Es necesario estandarizar los datos? Muestre el conjunto de variables a estandarizar.\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> __Nota__:\n",
        "Las clases en SciKit para preprocesar los datos numéricos y categóricos son [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) y [`LabelBinarizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html), respectivamente.\n",
        "<div>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Respuesta:**\n",
        "\n",
        "Las variables numéricas a estandarizar son:\n",
        "\n",
        "* **age:** Edad\n",
        "* **fnlwgt:** Peso final\n",
        "* **education-num:** Nivel de educación en número de años\n",
        "* **capital-gain:** Ganancias de capital\n",
        "* **capital-loss:** Pérdidas de capital\n",
        "* **hours-per-week:** Horas de trabajo por semana"
      ],
      "metadata": {
        "id": "We4Xr3q32KLY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1e0yCFlDQ4a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_0lHskwDQ4b"
      },
      "source": [
        "Finalmente, para poder utilizar distintos métodos de preprocesamiento sobre el mismo dataset en un Pipeline, tenemos que definir nuestra propia clase de transformación para gestionar los distintos tipos de datos. También se podría hacer con la clase [`FeatureUnion`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html), gestionando cada tipo de datos con un pipeline distinto y luego uniéndolos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuPkJM7VDQ4c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11Lv-_v6DQ4c"
      },
      "source": [
        "Con esto ya hemos definido la transformación de preprocesado que podremos usar en nuestro pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlchYZD9DQ4c"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> __Nota__: La función [`get_dummies`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) de pandas, por defecto ignora los NaN al hacer el one-hot encoding.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwuYVNaTDQ4d"
      },
      "source": [
        "<div style=\"text-align: right\">\n",
        "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></font></a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBhIPUBzDQ4d"
      },
      "source": [
        "---\n",
        "\n",
        "<a id=\"section3\"></a>\n",
        "## <font color=\"#004D7F\"> 3. Fase de modelado</font>\n",
        "\n",
        "En SciKit hay multitud de modelos de aprendizaje supervisado ya implementados (https://scikit-learn.org/stable/supervised_learning.html) Algunos de estos modelos son:\n",
        "\n",
        "1. [Regresión logística](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\n",
        "1. [$k$-vecinos más cercanos ($k$-NN)](https://scikit-learn.org/stable/modules/neighbors.html)\n",
        "1. [Árboles de decisión](https://scikit-learn.org/stable/modules/tree.html)\n",
        "1. [Support Vector Machine (SVM)](https://scikit-learn.org/stable/modules/svm.html)\n",
        "1. [Perceptrón multicapa](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)\n",
        "\n",
        "Algunos de estos modelos son muy sensibles a ciertos aspectos del preprocesamiento de los datos, por ejemplo, al escalado/normalización de las variables, a la codificación de variables categóricas o a la secuencialidad de los datos de entrada. Por lo que es conveniente tener en cuenta estos aspectos en todas las desiciones del proceso, desde el preprocesamiento de los datos a la selección modelos e hiperparámetros. **En la documentación de SciKit para cada uno de los modelos podemos encontrar instrucciones que nos indican/recuerdan estas cuestiones.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encontrar el mejor modelo que nos dé el mejor rendimiento con el mejor proceso establecido en los pasos anteriores (Utilize por lo menos 3 técnicas)"
      ],
      "metadata": {
        "id": "d0mdFyTm4FTp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10oWtxQWDQ4d"
      },
      "outputs": [],
      "source": [
        "## usar pipeline\n",
        "\n",
        "\n",
        "results_models=[]\n",
        "numeric_features = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
        "categorical_features = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
        "\n",
        "\n",
        "#\n",
        "# Lista de modelos e hiperparámetros\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000)\n",
        "    ,    'k-NN': KNeighborsClassifier()\n",
        "    ,    'Decision Tree': DecisionTreeClassifier()\n",
        "    ,'Random Forest': RandomForestClassifier(n_estimators=100)\n",
        "    ,'XGBoost': XGBClassifier()\n",
        "    ,    'SVM': SVC()\n",
        "    ,    'MLP': MLPClassifier(max_iter=1000)\n",
        "}\n",
        "\n",
        "\n",
        "# Definir la cuadrícula de parámetros para GridSearchCV para cada modelo\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'classifier__C': [0.1, 1.0, 10.0]\n",
        "    },\n",
        "    'k-NN': {\n",
        "        'classifier__n_neighbors': [3, 5, 7]\n",
        "    },\n",
        "    'Decision Tree': {\n",
        "        'classifier__max_depth': [None, 10, 20]\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'classifier__n_estimators': [100, 200, 300],\n",
        "        'classifier__max_depth': [None, 10, 20]\n",
        "    },\n",
        "    'SVM': {\n",
        "        'classifier__C': [0.1, 1.0, 10.0],\n",
        "        'classifier__kernel': ['linear', 'rbf']\n",
        "    },\n",
        "    'MLP': {\n",
        "        'classifier__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "        'classifier__alpha': [0.0001, 0.001]\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# Entrenar y evaluar cada modelo usando GridSearchCV\n",
        "for model_name in models:\n",
        "\n",
        "    pipeline = Pipeline(steps=[\n",
        "          ('preprocessor', DataPreprocessor(numeric_cols=numeric_features,\n",
        "                                   categorical_cols=categorical_features)),\n",
        "            (\"classifier\", models[model_name])\n",
        "    ])\n",
        "\n",
        "    grid_search = GridSearchCV(pipeline, param_grids[model_name], cv=5)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Modelo: {model_name}\")\n",
        "    print(\"Mejores parámetros encontrados: \", grid_search.best_params_)\n",
        "    print(\"Mejor puntuación de validación cruzada: \", grid_search.best_score_)\n",
        "\n",
        "    # Evaluar el modelo en el conjunto de prueba\n",
        "    test_score = grid_search.score(X_test, y_test)\n",
        "    print(\"Puntuación en el conjunto de prueba: \", test_score)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    results_models.append([model_name, grid_search.best_params_, grid_search.best_score_, test_score])\n",
        "\n",
        "\n",
        "print(\"Resultados de los Modelos\")\n",
        "print (results_models)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Mejor Modelo\")\n",
        "best_model_info = best_model(results_models)\n",
        "if best_model_info:\n",
        "    best_model_name, best_model_parameters, best_model_train_score ,  best_model_test_score = best_model_info\n",
        "    print(f\"El mejor modelo es: {best_model_name} con una puntuación en el conjunto de prueba de: {best_model_test_score}\")\n",
        "else:\n",
        "    print(\"No se pudo determinar el mejor modelo.\")\n",
        "\n",
        "best_model = models[best_model_name]\n",
        "\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyTvtV_2DQ4f"
      },
      "source": [
        "Una vez que hemos seleccionado los mejores parámetros utilizando unicamente el conjunto de entrenamiento, podemos evaluar con el conjunto de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arcXayRJDQ4f"
      },
      "outputs": [],
      "source": [
        "## usar pipeline\n",
        "\n",
        "print(\"Preparamos el Dataset Test\")\n",
        "\n",
        "\n",
        "numeric_features = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
        "categorical_features = ['age-binned', 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
        "\n",
        "\n",
        "test_data['age'] = test_data['age'].replace('|1x3 Cross validator', np.nan)\n",
        "test_data['age'] = test_data['age'].astype(float)\n",
        "test_data = handle_missing_values(test_data)\n",
        "test_data = age_binning(test_data)\n",
        "\n",
        "\n",
        "#Corregimos variable respuesta\n",
        "test_data[\"income\"].fillna(\"<=50K.\", inplace=True)\n",
        "test_data['income'].replace('<=50K.', '<=50K', inplace=True)\n",
        "test_data['income'].replace('>50K.', '>50K', inplace=True)\n",
        "\n",
        "\n",
        "X = test_data.drop(\"income\", axis=1)\n",
        "\n",
        "# Binarizamos la variable respuesta\n",
        "print (\"Binarizamos la variable respuesta\")\n",
        "print(\"Pre Binarizar\")\n",
        "print(test_data[\"income\"].unique())\n",
        "test_data[\"income\"].unique()\n",
        "\n",
        "y = test_data[\"income\"].apply(lambda x: 1 if x == '>50K' else 0)\n",
        "print(\"Post Binarizar\")\n",
        "test_data[\"income\"].unique()\n",
        "\n",
        "# Dividir datos en entrenamiento y prueba\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "print(\"Aplicamos la funcion de Undersampling para balancear el dataset\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = UnderSampling_data(X, y, sampling_strategy = 0.8, test_size=0.3 )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pipeline = Pipeline(steps=[\n",
        "      ('preprocessor', DataPreprocessor(numeric_cols=numeric_features,\n",
        "                                categorical_cols=categorical_features)),\n",
        "        (\"classifier\", best_model)])\n",
        "\n",
        "\n",
        "\n",
        "pipeline.set_params(**best_model_parameters)\n",
        "\n",
        " pipeline.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "y_predictions = pipeline.predict(X_test)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Aplicamos el  mejor modelo en el Dataset Test\")\n",
        "print(\"\\n\")\n",
        "# Evaluar métricas\n",
        "accuracy = accuracy_score(y_test, y_predictions)\n",
        "precision = precision_score(y_test, y_predictions)\n",
        "recall = recall_score(y_test, y_predictions)\n",
        "f1 = f1_score(y_test, y_predictions)\n",
        "roc_auc = roc_auc_score(y_test, y_predictions)\n",
        "\n",
        "print(f\"Model: {best_model}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.4f}\\n\")\n",
        "\n",
        "print(\"Evaluamos resultados del  mejor modelo en el Dataset Test\")\n",
        "evaluate_model(best_model_name, pipeline, y_test, y_predictions)"
      ],
      "metadata": {
        "id": "3uZrNpwhKf1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section3\"></a>\n",
        "## <font color=\"#004D7F\"> 4. Conclusiones</font>"
      ],
      "metadata": {
        "id": "S8ha10TT4-xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Ingrese aquí sus comentarios finales sobre los hallazgos relevantes"
      ],
      "metadata": {
        "id": "Oe3VQisd5sUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OCZlZjOZ57tm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGYVIpR-DQ4k"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### <font color=\"#004D7F\"> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Ejercicio (opcional)</font>\n",
        "\n",
        "Nos podríamos haber planteado otras cuestiones tanto durante el preprocesamiento de los datos como en las fases posteriores de ajuste de parámetros y clasificación. Dejamos aquí algunas sugerencias para que pruebes e intentes mejorar la tasa de aciertos:\n",
        "\n",
        "* Binarizar las variables numéricas. Por ejemplo, crear grupos de edad.\n",
        "* Estudiar la relación entre variables. ¿Se puede suprimir/añadir alguna?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O53TEh8lDQ4k"
      },
      "source": [
        "<div style=\"text-align: left\"><font size=4> <i class=\"fa fa-check-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i></font></div>\n",
        "\n",
        "<div style=\"text-align: right\">\n",
        "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></font></a>\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-coffee\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> </font></div>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}