{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicardoCruzPaulino/test-repo/blob/master/Evaluaci%C3%B3n_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3UYA16DDQ3-"
      },
      "source": [
        "\n",
        "<h2><font color=\"#004D7F\" size=4>Módulo 4</font></h2>\n",
        "\n",
        "\n",
        "\n",
        "<h1><font color=\"#004D7F\" size=5>Evaluación conocimientos</font></h1>\n",
        "\n",
        "<br>\n",
        "<div style=\"text-align: right\">\n",
        "<font color=\"#004D7F\" size=3>Asociación Popular de Ahorro y Préstamos - APAP</font><br>\n",
        "<font color=\"#004D7F\" size=3>Prevención y Contro de Fraude\n",
        "</font><br>\n",
        "<font color=\"#004D7F\" size=3>Setiembre 2023</font>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWij6XOkDQ4J"
      },
      "source": [
        "---\n",
        "\n",
        "<a id=\"indice\"></a>\n",
        "<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n",
        "\n",
        "\n",
        "* [1. Introducción](#section1)\n",
        "* [2. Preparando los datos](#section2)\n",
        "   * [Análisis de datos](#section21)\n",
        "   * [Preprocesamiento](#section22)\n",
        "* [3. Fase de modelado](#section3)\n",
        "   * [Gradiente descendiente estocástico (SGD)](#section33)\n",
        "   * [Regresión logística](#section31)\n",
        "   * [Support Vector Machine (SVM)](#section32)\n",
        "   * [Árboles de decision](#section34)\n",
        "* [4. Conclusiones](#section5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQVAZTzUDQ4L"
      },
      "source": [
        "---\n",
        "\n",
        "<a id=\"section1\"></a>\n",
        "## <font color=\"#004D7F\"> 1. Introducción</font>\n",
        "\n",
        "La presente evaluación tiene como finalidad medir el conocimiento de la herramienta y el proceso de construcción de modelos ya sea de clasificación o predictivos.\n",
        "\n",
        "En esta evaluación se va a utilizar el dataset [Census Income Dataset](http://archive.ics.uci.edu/ml/datasets/Census+Income). Este dataset contiene datos como la edad, trabajo, estudios, etc. de más de 48K personas.\n",
        "\n",
        "El objetivo consiste en predecir si dicha persona tiene unos ingresos que superan los 50K dólares anuales; para ello deberá guiarse a través de la metodología CRISP-DM para llevar a cabo el objetivo esperado.\n",
        "\n",
        "\n",
        "<div style=\"text-align: right\">\n",
        "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></font></a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oouNqeDBpHwk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meDpMtgxDQ4N"
      },
      "source": [
        "---\n",
        "\n",
        "<a id=\"section2\"></a>\n",
        "## <font color=\"#004D7F\"> 2. Preparando los datos</font>\n",
        "\n",
        "\n",
        "Cargar el dataset indicado y las librerias necesarias para analizar los datos de entrada. Se solicita:\n",
        "\n",
        "* ¿Que tipos de datos encontramos en el dataset?\n",
        "* ¿La variable respuesta esta balanceada?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uv6Oxt6dDQ4P"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, LabelBinarizer, MultiLabelBinarizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Cargar el dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "column_names = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \"occupation\",\n",
        "                \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"]\n",
        "data = pd.read_csv(url, names=column_names, sep=',\\s', engine='python')\n",
        "\n",
        "url_test = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
        "\n",
        "test_data = pd.read_csv(url_test, names=column_names, sep=',\\s', engine='python')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clases y Funciones Utilitarias\n",
        "\n",
        "\n",
        "\n",
        "#Clases Utilitarias de Transformaciones\n",
        "\n",
        "\n",
        "class LabelBinarizerForPipeline(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Extends LabelBinarizer to work in pipelines for both single and multi-output scenarios.\n",
        "    \"\"\"\n",
        "    def __init__(self, sparse_output=False):\n",
        "        self.sparse_output = sparse_output\n",
        "        self.encoder = None  # Initialize encoder as None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fits the encoder based on the shape of the input data.\n",
        "        Uses MultiLabelBinarizer for 2D input (multi-output) and LabelBinarizer for 1D input (single-output).\n",
        "        \"\"\"\n",
        "        if X.ndim == 2:  # Check if input is 2D (multi-output)\n",
        "            self.encoder = MultiLabelBinarizer(sparse_output=self.sparse_output)\n",
        "        else:\n",
        "            self.encoder = LabelBinarizer(sparse_output=self.sparse_output)\n",
        "        self.encoder.fit(X)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Transforms the input data using the fitted encoder.\"\"\"\n",
        "        return self.encoder.transform(X)\n",
        "\n",
        "\n",
        "# Clase personalizada para preprocesamiento\n",
        "class DataPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, numeric_cols, categorical_cols):\n",
        "        \"\"\"\n",
        "        Inicializa el preprocesador con las columnas numéricas y categóricas.\n",
        "\n",
        "        Args:\n",
        "            numeric_cols (list): Lista de nombres de columnas numéricas\n",
        "            categorical_cols (list): Lista de nombres de columnas categóricas\n",
        "        \"\"\"\n",
        "        self.numeric_cols = numeric_cols\n",
        "        self.categorical_cols = categorical_cols\n",
        "        self.preprocessor = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Ajusta el preprocesador a los datos.\"\"\"\n",
        "        # Definimos el ColumnTransformer con los transformadores específicos\n",
        "        self.preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', StandardScaler(), self.numeric_cols),          # Escala datos numéricos\n",
        "           #     ('cat', LabelBinarizerForPipeline(), self.categorical_cols)       # Binariza datos categóricos\n",
        "\n",
        "                 (\"onehot\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), self.categorical_cols)\n",
        "            ],\n",
        "            remainder='passthrough'  # Mantiene las columnas no especificadas sin cambios\n",
        "        )\n",
        "        self.preprocessor.fit(X,y)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transforma los datos usando el preprocesador ajustado.\"\"\"\n",
        "        return self.preprocessor.transform(X)\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Ajusta y transforma los datos en un solo paso.\"\"\"\n",
        "        return self.fit(X, y).transform(X)\n",
        "\n",
        "\n",
        "\n",
        "###  Funciones\n",
        "def evaluate_model(name, model, X_test, y_test):\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Model: {name}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}\\n\")\n",
        "\n",
        "# Evaluación del mejor modelo en el conjunto de prueba\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    return accuracy, precision, recall, f1, roc_auc\n",
        "\n",
        "\n",
        "def handle_missing_values(data):\n",
        "  # Reemplazar valores nulos por NaN\n",
        "  data.replace('?', np.nan, inplace=True)\n",
        "  # Imputar valores perdidos en variables numéricas con la mediana\n",
        "  numeric_imputer = SimpleImputer(strategy='median')\n",
        "  data[['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']] = numeric_imputer.fit_transform(data[['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']])\n",
        "\n",
        "  # Imputar valores perdidos en variables categóricas con la moda\n",
        "  categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "  data[['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']] = categorical_imputer.fit_transform(data[['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']])\n",
        "  return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def EDA_data(data):\n",
        "  # 1. Boxplots para identificar outliers univariados\n",
        "  print(\"# 1. Boxplots para identificar outliers univariados\")\n",
        "\n",
        "  for col in data.select_dtypes(include=['int64', 'float64']).columns:\n",
        "      plt.figure(figsize=(10, 4))\n",
        "      sns.boxplot(x=data[col])\n",
        "      plt.title(f'Boxplot for {col}')\n",
        "      plt.show()\n",
        "\n",
        "  print(\"# 2. Histogramas para identificar outliers univariados\")\n",
        "  # 2. Histogramas para identificar outliers univariados\n",
        "  for col in data.select_dtypes(include=['int64', 'float64']).columns:\n",
        "      plt.figure(figsize=(10, 4))\n",
        "      sns.histplot(data[col], kde=True)\n",
        "      plt.title(f'Histogram for {col}')\n",
        "      plt.show()\n",
        "\n",
        "  print(\"# 3. Scatter plots para identificar relaciones multivariadas (ejemplo con age y hours-per-week)\")\n",
        "  # 3. Scatter plots para identificar relaciones multivariadas (ejemplo con age y hours-per-week)\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  sns.scatterplot(x=data['age'], y=data['hours-per-week'])\n",
        "  plt.title('Scatter plot of Age vs Hours per Week')\n",
        "  plt.xlabel('Age')\n",
        "  plt.ylabel('Hours per Week')\n",
        "  plt.show()\n",
        "\n",
        "  print(\"# 4. Matriz de correlación para identificar relaciones inusuales\")\n",
        "  # 4. Matriz de correlación para identificar relaciones inusuales\n",
        "  corr = data.select_dtypes(include=['int64', 'float64']).corr()\n",
        "  sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "  plt.title('Correlation Matrix')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def age_binning(data):\n",
        "  bins = np.linspace(min(data[\"age\"]), max(data[\"age\"]), 4)\n",
        "  bins\n",
        "  group_names = ['Young', 'Adult', 'Elder']\n",
        "\n",
        "  data['age-binned'] = pd.cut(data['age'], bins, labels=group_names, include_lowest=True )\n",
        "  data[['age','age-binned']].head(20)\n",
        "  return data\n",
        "\n",
        "def plot_tabla_contingencia(data, column):\n",
        "# Variables categóricas\n",
        "  categorical_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "  # Crear tablas de contingencia\n",
        "  for column in categorical_columns:\n",
        "      if column != 'income':\n",
        "          contingency_table = pd.crosstab(data[column], data['income'])\n",
        "          print(f\"Tabla de Contingencia para {column} y income:\")\n",
        "          print(contingency_table)\n",
        "          print(\"\\n\")\n",
        "\n",
        "\n",
        "def plot_matriz_correlaciones(data):\n",
        "\n",
        "# Matriz de correlación\n",
        "corr = data.select_dtypes(include=['int64', 'float64']).corr()\n",
        "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "def plot_histograms(data):\n",
        "# Histogramas para variables numéricas\n",
        "for col in data.select_dtypes(include=['int64', 'float64']).columns:\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.histplot(data[col], kde=True)\n",
        "    plt.title(f'Histogram for {col}')\n",
        "    plt.show()\n",
        "\n",
        "# Balancear Muestra aplicando UnderSampling datos en conjuntos de entrenamiento y prueba\n",
        "def UnderSampling_data(X, y, sampling_strategy = 0.8):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  random_under_sampler = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
        "  X_train_res, y_train_res = random_under_sampler.fit_resample(X_train, y_train)\n",
        "  X_test_res, y_test_res = random_under_sampler.fit_resample(X_test, y_test)\n",
        "  return X_test_res, y_test_res , X_train_res, y_train_res\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I34IqLTIKN9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **¿Que tipos de datos encontramos en el dataset?**"
      ],
      "metadata": {
        "id": "NlQ09ZukTXDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Respuesta:** Al momnento de ejecucion identificamos que los siguientes tipos datos.\n",
        "*  Numerico Entero == > int64  (6 variables)\n",
        "*   Texto ==>  object   (8 variables)"
      ],
      "metadata": {
        "id": "fW_8A4bDTYI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tipo de Datos\")\n",
        "print(data.dtypes)"
      ],
      "metadata": {
        "id": "tKYYG7GuTyYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**¿La variable respuesta esta balanceada?**"
      ],
      "metadata": {
        "id": "zAY31TYjTp7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Repsuesta:** La variable respuesta \"target\" no se encuentra balanceads .\n",
        "*  **<=50K:**  Con 24,720 valores para 75.92%\n",
        "*   **>50K:** Con 7,841 valores para 24.08%"
      ],
      "metadata": {
        "id": "67edBVYRTrO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribución de la variable objetivo\n",
        "income_counts = data['income'].value_counts()\n",
        "print(income_counts)\n",
        "\n",
        "# Proporción de cada clase\n",
        "income_proportion = income_counts / income_counts.sum()\n",
        "print(income_proportion)"
      ],
      "metadata": {
        "id": "IifZrCjNUByW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "income_counts = test_data['income'].value_counts()\n",
        "print(income_counts)\n",
        "income_counts.plot(kind=\"bar\")"
      ],
      "metadata": {
        "id": "HdFwW723UDQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O-fbCVzDQ4T"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "<a id=\"section21\"></a>\n",
        "### <font color=\"#004D7F\">Análisis de datos</font>\n",
        "\n",
        "Realizar un análisis exploratorio de los datos. Responder a las siguientes preguntas (Justifique su respuesta):\n",
        "\n",
        "* ¿Estan las muestras balanceadas?\n",
        "* ¿De qué tipo son los datos? ¿Hay que transformarlos?\n",
        "* ¿Hay valores perdidos? Lleve a cabo el tratamiento adecuado según su criterio para tratar con dichos valores. Justifique su respuesta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-A_M3E2EDQ4U"
      },
      "outputs": [],
      "source": [
        "print(data.dtypes)\n",
        "\n",
        "# Distribución de la variable objetivo\n",
        "income_counts = data['income'].value_counts()\n",
        "print(income_counts)\n",
        "\n",
        "# Proporción de cada clase\n",
        "income_proportion = income_counts / income_counts.sum()\n",
        "print(income_proportion)\n",
        "\n",
        "\n",
        "# Revisar los tipos de datos\n",
        "print(data.dtypes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "missing_values = data.isnull().sum()\n",
        "print(missing_values)\n",
        "\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "\n",
        "missing_values = data.isnull().sum()\n",
        "print(missing_values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**¿Estan las muestras balanceadas?**"
      ],
      "metadata": {
        "id": "-dj2i_rDSsoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Respuesta:** Dado que la variable respuesta no esta balanceada, concluimos que las muestras no estan balanceadas. Agregamos gráfico para su comprobación."
      ],
      "metadata": {
        "id": "fsOj91kkVMGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "income_counts = data['income'].value_counts()\n",
        "print(income_counts)\n",
        "income_counts.plot(kind=\"bar\")"
      ],
      "metadata": {
        "id": "477akZxFVXLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**¿De qué tipo son los datos? ¿Hay que transformarlos?**"
      ],
      "metadata": {
        "id": "ASeWDSOFVoZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Respuesta:**\n",
        "\n",
        "**Tipos de Variables de datos**\n",
        "*   **Numéricas:** age, fnlwgt, education-num, capital-gain, capital-loss, hours-per-week.\n",
        "*   **Categóricas:** workclass, education, marital-status, occupation, relationship, race, sex, native-country.\n",
        "\n",
        "**Transformación para Variables de datos**\n",
        "*   **Numéricas:**  Aplicar Estandarizacion / Escalado, segun criterio seleccionado.\n",
        "*   **Categóricas:**   Aplicar One-Hot Encoding / Label Encoding, segun aplique para el modelo.\n"
      ],
      "metadata": {
        "id": "TWYONmhcS3sA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GepWPl51WOtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**¿Hay valores perdidos? Lleve a cabo el tratamiento adecuado según su criterio para tratar con dichos valores. Justifique su respuesta**"
      ],
      "metadata": {
        "id": "qjpzTCJ7WPE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Respuesta:**\n",
        "Al momento de realizar la ejecucion\n",
        "las siguientes variables presentan valores perdidos:\n",
        "\n",
        "1.   workclass         1836\n",
        "2.   occupation        1843\n",
        "3.   native-country     583\n",
        "\n",
        "\n",
        "Dado que son variable categoricas, las reemplazaremos con la moda."
      ],
      "metadata": {
        "id": "g0_w9OwGWRZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Revision datos perdidos\")\n",
        "missing_values = data.isnull().sum()\n",
        "print(missing_values)\n",
        "\n",
        "print(\"Corrección de datos perdidos\")\n",
        "data = handle_missing_values(data)\n",
        "missing_values = data.isnull().sum()\n",
        "print(missing_values)\n"
      ],
      "metadata": {
        "id": "qN4fwipAWiKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Existen outliers o valores atípicos? Muestre 2 indicadores (univariados y multivariados) para identificar la existencia de dichos valores. En caso existan, aplique el tratamiento adecuado según su criterio."
      ],
      "metadata": {
        "id": "QKlyeuR2rufq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Respuesta:**\n",
        "\n",
        "**Indicadores Univariados**\n",
        " Para cada variable numerica presentaremos\n",
        "\n",
        "1.   Boxplots\n",
        "2.   Histogramas\n",
        "\n",
        "**Indicadores Multivariados**\n",
        "Combinaremos la variable respuesta con todas las variables  numericas.\n",
        "\n",
        "1.   Gráficos de dispersión (scatter plots)\n",
        "2.   Matriz de correlación:\n"
      ],
      "metadata": {
        "id": "e_NSj_wDXTrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "#**Indicadores Univariados**\n",
        "print(\"**Indicadores Univariados**\")\n",
        "for col in data.select_dtypes(include=['int64', 'float64']).columns:\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.boxplot(x=data[col])\n",
        "    plt.title(f'Boxplot for {col}')\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.histplot(data[col], kde=True)\n",
        "    plt.title(f'Histogram for {col}')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#**Indicadores Multivariados**\n",
        "print(\"**Indicadores Multivariados**\")\n",
        "# Gráficos de dispersión (scatter plots\n",
        "for col in data.select_dtypes(include=['int64', 'float64']).columns:\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  sns.scatterplot(x=data[{col}], y=data['income'])\n",
        "  plt.title('Scatter plot of Age vs Hours per Week')\n",
        "  plt.xlabel('Age')\n",
        "  plt.ylabel('Hours per Week')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "for col in data.select_dtypes(include=['int64', 'float64']).columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.scatterplot(x=data[col], y=data['income'])\n",
        "    plt.title(f'Scatter plot of {col} vs Income')\n",
        "    plt.xlabel(f'{col}')\n",
        "    plt.ylabel('Income')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# 4. Matriz de correlación para identificar relaciones inusuales\n",
        "corr = data.select_dtypes(include=['int64', 'float64']).corr()\n",
        "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BIyQXhxEsBLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkZt_XBmDQ4X"
      },
      "source": [
        "\n",
        "LLeve a cabo un análisis descriptivo mediante tablas y gráficos (univariados y bivariados) Ejemplo: Histogramas, matriz de correlaciones, etc. Comente sus hallazgos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "zw52aB2PDQ4Y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Cargar el dataset (asumiendo que ya se ha limpiado y preprocesado)\n",
        "# data = pd.read_csv(...) # Asumiendo que 'data' es el DataFrame con los datos preprocesados\n",
        "\n",
        "# Histogramas para variables numéricas\n",
        "for col in data.select_dtypes(include=['int64', 'float64']).columns:\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.histplot(data[col], kde=True)\n",
        "    plt.title(f'Histogram for {col}')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "for col in data.select_dtypes(include=['object']).columns:\n",
        "print(f\"Frequency Table for {col}:\")\n",
        "print(data[col].value_counts())\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# Matriz de correlación\n",
        "corr = data.select_dtypes(include=['int64', 'float64']).corr()\n",
        "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDOtsRP8DQ4Z"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> __Nota__: Si tuviesemos que imputar valores a alguna columna, se podría utilizar el valor medio o mediana para las variables numéricas y el valor más frecuente para las categóricas.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXOwMi3vDQ4Z"
      },
      "source": [
        "---\n",
        "\n",
        "<a id=\"section22\"></a>\n",
        "### <font color=\"#004D7F\">Preprocesamiento</font>\n",
        "\n",
        "Antes de realizar las transformaciones necesarias tenemos que realizar la partición entre muestras de train y test. LLeve a cabo dicha partición."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_AyjcFGDQ4Z"
      },
      "outputs": [],
      "source": [
        "X = data.drop(\"income\", axis=1)\n",
        "###########  y = data[\"income\"]\n",
        "\n",
        "y = data[\"income\"].apply(lambda x: 1 if x == '>50K' else 0)  # Convertir a binario\n",
        "\n",
        "# Dividir datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "data[\"income\"].unique()\n",
        "#y = data[\"income\"].apply(lambda x: 1 if x == \" >50K\" else 0)\n",
        "y.unique()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6EbSeENDQ4a"
      },
      "source": [
        "¿Es necesario estandarizar los datos? Muestre el conjunto de variables a estandarizar.\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> __Nota__:\n",
        "Las clases en SciKit para preprocesar los datos numéricos y categóricos son [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) y [`LabelBinarizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html), respectivamente.\n",
        "<div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1e0yCFlDQ4a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_0lHskwDQ4b"
      },
      "source": [
        "Finalmente, para poder utilizar distintos métodos de preprocesamiento sobre el mismo dataset en un Pipeline, tenemos que definir nuestra propia clase de transformación para gestionar los distintos tipos de datos. También se podría hacer con la clase [`FeatureUnion`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html), gestionando cada tipo de datos con un pipeline distinto y luego uniéndolos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuPkJM7VDQ4c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11Lv-_v6DQ4c"
      },
      "source": [
        "Con esto ya hemos definido la transformación de preprocesado que podremos usar en nuestro pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlchYZD9DQ4c"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> __Nota__: La función [`get_dummies`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) de pandas, por defecto ignora los NaN al hacer el one-hot encoding.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwuYVNaTDQ4d"
      },
      "source": [
        "<div style=\"text-align: right\">\n",
        "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></font></a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBhIPUBzDQ4d"
      },
      "source": [
        "---\n",
        "\n",
        "<a id=\"section3\"></a>\n",
        "## <font color=\"#004D7F\"> 3. Fase de modelado</font>\n",
        "\n",
        "En SciKit hay multitud de modelos de aprendizaje supervisado ya implementados (https://scikit-learn.org/stable/supervised_learning.html) Algunos de estos modelos son:\n",
        "\n",
        "1. [Regresión logística](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\n",
        "1. [$k$-vecinos más cercanos ($k$-NN)](https://scikit-learn.org/stable/modules/neighbors.html)\n",
        "1. [Árboles de decisión](https://scikit-learn.org/stable/modules/tree.html)\n",
        "1. [Support Vector Machine (SVM)](https://scikit-learn.org/stable/modules/svm.html)\n",
        "1. [Perceptrón multicapa](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)\n",
        "\n",
        "Algunos de estos modelos son muy sensibles a ciertos aspectos del preprocesamiento de los datos, por ejemplo, al escalado/normalización de las variables, a la codificación de variables categóricas o a la secuencialidad de los datos de entrada. Por lo que es conveniente tener en cuenta estos aspectos en todas las desiciones del proceso, desde el preprocesamiento de los datos a la selección modelos e hiperparámetros. **En la documentación de SciKit para cada uno de los modelos podemos encontrar instrucciones que nos indican/recuerdan estas cuestiones.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encontrar el mejor modelo que nos dé el mejor rendimiento con el mejor proceso establecido en los pasos anteriores (Utilize por lo menos 3 técnicas)"
      ],
      "metadata": {
        "id": "d0mdFyTm4FTp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10oWtxQWDQ4d"
      },
      "outputs": [],
      "source": [
        "## usar pipeline\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "results=[]\n",
        "numeric_features = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
        "categorical_features = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
        "\n",
        "\n",
        "#\n",
        "# Lista de modelos e hiperparámetros\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000)\n",
        "    #,    'k-NN': KNeighborsClassifier()\n",
        "    #,    'Decision Tree': DecisionTreeClassifier()\n",
        "   # ,    'SVM': SVC()\n",
        "   # ,    'MLP': MLPClassifier(max_iter=1000)\n",
        "}\n",
        "\n",
        "\n",
        "# Definir la cuadrícula de parámetros para GridSearchCV para cada modelo\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'classifier__C': [0.1, 1.0, 10.0]\n",
        "    },\n",
        "    'k-NN': {\n",
        "        'classifier__n_neighbors': [3, 5, 7]\n",
        "    },\n",
        "    'Decision Tree': {\n",
        "        'classifier__max_depth': [None, 10, 20]\n",
        "    },\n",
        "    'SVM': {\n",
        "        'classifier__C': [0.1, 1.0, 10.0],\n",
        "        'classifier__kernel': ['linear', 'rbf']\n",
        "    },\n",
        "    'MLP': {\n",
        "        'classifier__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "        'classifier__alpha': [0.0001, 0.001]\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# Entrenar y evaluar cada modelo usando GridSearchCV\n",
        "for model_name in models:\n",
        "\n",
        "    pipeline = Pipeline(steps=[\n",
        "          ('preprocessor', DataPreprocessor(numeric_cols=numeric_features,\n",
        "                                   categorical_cols=categorical_features)),\n",
        "            (\"classifier\", models[model_name])\n",
        "    ])\n",
        "\n",
        "    grid_search = GridSearchCV(pipeline, param_grids[model_name], cv=5)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Modelo: {model_name}\")\n",
        "    print(\"Mejores parámetros encontrados: \", grid_search.best_params_)\n",
        "    print(\"Mejor puntuación de validación cruzada: \", grid_search.best_score_)\n",
        "\n",
        "    # Evaluar el modelo en el conjunto de prueba\n",
        "    test_score = grid_search.score(X_test, y_test)\n",
        "    print(\"Puntuación en el conjunto de prueba: \", test_score)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    results.append([model_name, grid_search.best_params_, grid_search.best_score_, test_score])\n",
        "\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyTvtV_2DQ4f"
      },
      "source": [
        "Una vez que hemos seleccionado los mejores parámetros utilizando unicamente el conjunto de entrenamiento, podemos evaluar con el conjunto de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arcXayRJDQ4f"
      },
      "outputs": [],
      "source": [
        "## usar pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section3\"></a>\n",
        "## <font color=\"#004D7F\"> 4. Conclusiones</font>"
      ],
      "metadata": {
        "id": "S8ha10TT4-xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Ingrese aquí sus comentarios finales sobre los hallazgos relevantes"
      ],
      "metadata": {
        "id": "Oe3VQisd5sUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OCZlZjOZ57tm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGYVIpR-DQ4k"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### <font color=\"#004D7F\"> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Ejercicio (opcional)</font>\n",
        "\n",
        "Nos podríamos haber planteado otras cuestiones tanto durante el preprocesamiento de los datos como en las fases posteriores de ajuste de parámetros y clasificación. Dejamos aquí algunas sugerencias para que pruebes e intentes mejorar la tasa de aciertos:\n",
        "\n",
        "* Binarizar las variables numéricas. Por ejemplo, crear grupos de edad.\n",
        "* Estudiar la relación entre variables. ¿Se puede suprimir/añadir alguna?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O53TEh8lDQ4k"
      },
      "source": [
        "<div style=\"text-align: left\"><font size=4> <i class=\"fa fa-check-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i></font></div>\n",
        "\n",
        "<div style=\"text-align: right\">\n",
        "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></font></a>\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-coffee\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> </font></div>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}